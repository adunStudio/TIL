{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "입력 신호, 즉 입력값 X에 가중치(W)를 곱하고 편향(b)을 더한 뒤\n",
    "활성화 함수(Sigmoid, ReLU 등)를 거쳐 결괏값 y를 만들어내는 것, 이것이 바\n",
    "로 인공 뉴런의 기본\n",
    "\n",
    "원하는 y 값을 만들어내기 위해 W와 b의 값\n",
    "을 변경해가면서 적절한 값을 찾아내는 최적화 과정을 학습(learning) 또는 훈련(training)이라 한다.\n",
    "\n",
    "Y = Sigmoide(x * w + b)\n",
    "출력 = 활성화 함수(입력 * 가중치 + 편향)(\n",
    "가중치와 편향을 찾는것이 학습\n",
    "\n",
    "활성화 함수(activation function)는 인공신경망을 통과해온 값을 최종적으로 어떤 값으로\n",
    "만들지를 결정합니다. 즉, 이 함수가 바로 인공 뉴런의 핵심 중에서도 가장 중요한 요소입니다.\n",
    "\n",
    "### 활성화 함수 종류\n",
    "- Sigmoid\n",
    "- ReLU\n",
    "- tanh\n",
    "\n",
    "최근에는 활성화 함수로 ReLU 함수를 많이 사용하는데, ReLU\n",
    "함수는 입력값이 0보다 작으면 항상 0을, 0보다 크면 입력값을 그대로 출력합니\n",
    "다.\n",
    "\n",
    "\n",
    "다시 정리하자면, 인공 뉴런은 가중치와 활성화 함수의 연결로 이뤄진 매우 간\n",
    "단한 구조입니다. 놀라운 점은, 이렇게 간단한 개념의 인공 뉴런을 충분히 많이\n",
    "연결해놓는 것만으로 인간이 인지하기 어려운 매우 복잡한 패턴까지도 스스로 학\n",
    "습할 수 있게 된다는 사실\n",
    "\n",
    "\n",
    "초기에는 w,b 조합을 일일일 변경해가는 계산이 매우 시간이 오래걸려 제대로 훈련시키기 어려웠으나\n",
    "'제한된 볼트만 머신'이라는 신경망 학습 알고리즘과 함께 다시 주목받기 시작\n",
    "그 후 '드롭아웃 기법', 'ReLU'등의 활성화 함수들이 개발되면서 딥러닝은 급속한 발전\n",
    "\n",
    "이렇게 발전해온 알고리즘 중심에는 '역전파(backpropagation)'가 있다.\n",
    "역전파는 간단하게 말해, 출력층이 내놓은 결과의 오차를 신경말을 따라 입력층까지 역으로 전파하면\n",
    "계산해나가는 방식\n",
    "이 방식은 입력층부터 가중치를 조절해\n",
    "가는 기존 방식보다 훨씬 유의미한 방식으로 가중치를 조절해줘서 최적화 과정이\n",
    "훨씬 빠르고 정확해집니다\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2)\n",
      "(6, 3)\n"
     ]
    }
   ],
   "source": [
    "# [털, 날개] 있으면 1, 없으면 0\n",
    "x_data = np.array([[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "print(x_data.shape)\n",
    "\n",
    "\"\"\"\n",
    "그다음은 각 개체가 실제 어떤 종류인지를 나타내는 레이블(분류값) 데이터를\n",
    "구성합니다. 즉, 앞서 정의한 특징 데이터의 각 개체가 포유류인지 조류인지, 아\n",
    "니면 제3의 종류인지를 기록한 실제 결괏값입니다.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "레이블 데이터는 원-핫 인코딩one-hot encoding이라는 특수한 형태로 구성합니다. 원핫 인코딩이란 데이터가 가질 수 있는 값들을 일렬로 나열한 배열을 만들고, 그중\n",
    "표현하려는 값을 뜻하는 인덱스의 원소만 1로 표기하고 나머지 원소는 모두 0으\n",
    "로 채우는 표기법입니다.\n",
    "\"\"\"\n",
    "# 기타   = [1, 0, 0]\n",
    "# 포유류  = [0, 1, 0]\n",
    "# 조류   = [0, 0, 1]\n",
    "\n",
    "y_data = np.array([\n",
    " [1, 0, 0], # 기타\n",
    " [0, 1, 0], # 포유류\n",
    " [0, 0, 1], # 조류\n",
    " [1, 0, 0],\n",
    " [1, 0, 0],\n",
    " [0, 0, 1]\n",
    "])\n",
    "print(y_data.shape)\n",
    "\n",
    "\"\"\"\n",
    "softmax 함수는 다음처럼 배열 내의 결괏값들을 전체 합이 1이 되도록 만들어\n",
    "줍니다. 전체가 1이니 각각은 해당 결과의 확률로 해석할 수 있습니다\n",
    "\"\"\"\n",
    "# [8.04, 2.76, -6.52] - > [0.53 0.24 0.23]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8833\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8700\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8573\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8450\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8332\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8219\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8111\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8007\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7906\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7810\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7717\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7627\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7541\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7458\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7378\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7301\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7226\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7154\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7084\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7017\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6952\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6889\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6828\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6769\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6712\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6657\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6603\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6551\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6500\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6451\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6403\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6356\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6311\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6267\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6224\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6182\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6142\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6102\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6063\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6025\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5988\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5952\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5916\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5882\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5848\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5815\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5782\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5751\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5719\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5689\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5659\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5629\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 975us/step - loss: 0.5600\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5572\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5544\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5516\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5489\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5463\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5437\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5411\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5386\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5361\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5336\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5312\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5288\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5264\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5241\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5218\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5195\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5173\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5151\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5129\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5107\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5086\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5065\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5044\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 972us/step - loss: 0.5024\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5003\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 949us/step - loss: 0.4983\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4963\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4944\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4924\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4905\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4886\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4867\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4848\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4829\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4811\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4793\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4775\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4757\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4739\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4722\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4704\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4687\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4670\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4653\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4636\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4620\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4603\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, input_dim = 2))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "history = model.fit(x_data, y_data,epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.4587\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_data,  y_data, verbose=2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}